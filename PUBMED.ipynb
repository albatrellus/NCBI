{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddfe6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#필요한 라이브러리 정의\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# 반복작업을 생략하기 위해 만든 함수. 하지만 실제 사용은 하지 않음\n",
    "def get_src():\n",
    "    src=browser.page_source\n",
    "    soup=BeautifulSoup(src,'lxml')\n",
    "    return soup\n",
    "\n",
    "# 검색란을 찾고 검색키를 보내는 함수\n",
    "def search_keys():\n",
    "    input_btn=browser.find_element_by_xpath('//*[@id=\"term\"]')\n",
    "    input_btn.send_keys('rRNA') # 이 부분이 검색키. 필요에 따라 수정 가능\n",
    "    search_btn=browser.find_element_by_xpath('//*[@id=\"search\"]')\n",
    "    search_btn.click()\n",
    "\n",
    "# 검색결과에서 균류 부분만 보는 함수\n",
    "def get_fungi_db():\n",
    "    src=browser.page_source\n",
    "    soup=BeautifulSoup(src,'lxml')\n",
    "    fungi_a=browser.find_element_by_xpath('//*[@id=\"_sp\"]/li/ul/li[3]/a')\n",
    "    fungi_a.click()\n",
    "\n",
    "# 다음 페이지로 넘어가는 함수. 다른 방법을 찾아서 쓰지 않음.    \n",
    "def get_more_info():\n",
    "    next_page=browser.find_elements_by_class_name('active page_link next')\n",
    "    for n in next_page:\n",
    "        n.click()\n",
    "\n",
    "def get_basic_info(): # 제목과 링크와 access 넘버를 받는 함수\n",
    "    urls=[]\n",
    "    titles=[]\n",
    "    accessions=[]\n",
    "    names=soup.find_all('p',attrs={'class':'title'}) # 제목(생물 종과 RNA 종류)을 모두 찾아 리스트 형태로 만듦\n",
    "    accesses=soup.find_all('dl',attrs={'class':'rprtid'}) # 마찬가지로 리스트 형태로 만듦\n",
    "    for i in range(len(names)): \n",
    "        name_p=names[i]\n",
    "        title=name_p.get_text()\n",
    "        titles.append(title)\n",
    "        \n",
    "        url=\"https://www.ncbi.nlm.nih.gov\"+name_p.find('a')['href'] # href 부분 자료로 주소 합성\n",
    "        urls.append(url)\n",
    "        \n",
    "        access=accesses[i].find('dd').get_text() # Accession 넘버는 <dl>태그 밑의 <dd> 태그에 존재\n",
    "        accessions.append(access)\n",
    "    return urls, titles, accessions\n",
    "\n",
    "def get_fasta():\n",
    "    Fastas=[]\n",
    "    max_sleep_time=5 # 최대 지연 시간\n",
    "    for i in range(len(urls)):\n",
    "        browser.get(urls[i]) # urls의 원소를 browser에서 엶.\n",
    "        src=browser.page_source\n",
    "        soup=BeautifulSoup(src,'lxml')\n",
    "        \n",
    "        '''실제 사용자처럼 지연 시간을 랜덤으로 주는 함수. 최소 1초부터 최대 5초까지 임의로 할당함. \n",
    "        지연 시간을 주지 않으면 로딩 시간이 조금 길 경우에만 제대로 크롤링할 수 있음. \n",
    "        사이트에서 데이터 유출을 막아두기 때문에 지연 시간은 불필요해보여도 없애면 안 됨'''\n",
    "        rand_value=random.randint(1,max_sleep_time) \n",
    "        time.sleep(rand_value)\n",
    "\n",
    "        Fasta_btn=browser.find_element_by_id('ReportShortCut6')\n",
    "        Fasta_btn.click()\n",
    "\n",
    "        rand_value=random.randint(1,max_sleep_time) # 마찬가지로 지연시간\n",
    "        time.sleep(rand_value)\n",
    "\n",
    "        src=browser.page_source\n",
    "        soup=BeautifulSoup(src,'lxml')\n",
    "\n",
    "        try:\n",
    "            Fasta=soup.find('pre').get_text() \n",
    "            Fastas.append(Fasta)\n",
    "        except:\n",
    "            Fasta='None' # try 부분에서 에러 발생 시 None 값 할당-> 불용데이터로 추후 지우거나 해야 함.\n",
    "            Fastas.append(Fasta)\n",
    "\n",
    "        print(i,Fasta) # 대용량 데이터 크롤링 시 작업 현황을 확인할 수 있도록 한 지표\n",
    "    return Fastas\n",
    "\n",
    "def count_none(): # 크롤링한 Fastas 리스트에서 불용 데이터 수 세기. 많으면 문제가 있다.\n",
    "    cnt=0\n",
    "    for f in Fastas:\n",
    "        if 'None' in f:\n",
    "            cnt+=1\n",
    "    print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f23bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''NCBI nucleotide 사이트로 접속'''\n",
    "chrome_driver=\"C:/Users/82105/COS/chromedriver.exe\"\n",
    "browser=webdriver.Chrome(chrome_driver)\n",
    "browser.implicitly_wait(3)\n",
    "db=\"https://www.ncbi.nlm.nih.gov/nuccore\"\n",
    "\n",
    "browser.get(db)\n",
    "src=browser.page_source\n",
    "soup=BeautifulSoup(src,'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eedd63",
   "metadata": {},
   "source": [
    "children은 부모보다 한 태그 아래에 있는 태그를 갖고 온다.\n",
    "descendatns는 부모보다 아래에 있는 태그를 모두 갖고 온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b03081f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "src=browser.page_source\n",
    "soup=BeautifulSoup(src,'lxml')\n",
    "search_keys()\n",
    "get_fungi_db() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4329dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "src=browser.page_source\n",
    "soup=BeautifulSoup(src,'lxml')\n",
    "\n",
    "'''20개씩 보기를 200개씩 보기로 바꿈'''\n",
    "tw_per_page=browser.find_element_by_xpath('/html/body/div[1]/div[1]/form/div[1]/div[4]/div/div[1]/ul/li[2]/a')\n",
    "tw_per_page.click()\n",
    "\n",
    "src=browser.page_source\n",
    "suop=BeautifulSoup(src,'lxml')\n",
    "\n",
    "browser.find_element_by_xpath('//*[@id=\"ps200\"]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8770c776",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''기본 정보를 크롤링한다'''\n",
    "src=browser.page_source\n",
    "soup=BeautifulSoup(src,'lxml')\n",
    "\n",
    "urls,titles,accessions=get_basic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19251259",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fastas=get_fasta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efad66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df=pd.DataFrame({'Species':titles, 'Url':urls, 'Accession':accessions, 'Fasta text':Fastas})\n",
    "result_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc9621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('Fungi rRNA Fasta Database.csv',encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8add611",
   "metadata": {},
   "outputs": [],
   "source": [
    "fungi_df=pd.read_csv('Fungi rRNA Accession Database.csv',encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924515e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "accession=fungi_df['Accession'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2919399",
   "metadata": {},
   "source": [
    "with open을 쓰면 간단하게 메모장에 저장할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c52219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Fungi Accession.txt','w', encoding='utf-8') as f:\n",
    "    for a in accession:\n",
    "        f.write(a+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
